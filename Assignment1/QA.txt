3.2.2 ###############################
  Q:  What info is being reported by the benchmark?
  A:  (This is purely obtained from checking out the output)
      1.Data of the Benchmark(Input and other default parameters)
      2.Multigrid matrices size and sparsity per levels: Operator and Interpolation Matrices
      2.1.  Operator complexity indicates how much memory is needed(if large then the matrix requires a lot of memory)
            Source: Page 4 of http://computation.llnl.gov/projects/hypre-scalable-linear-solvers-multigrid-methods/yang1.pdf
      3.Wallclock and CPU clock times for:
        -Setup Matrix and rhs
        -Preconditioned conjugate gradient Setup(PCG)
        -PCG Solve
      4.Final residual norm of the multigrid
      5.System Size * it / Solve Time
      **Additionally we can measure speedups with respect to different # of processes

  Q:  Which data is related to problem size, performance or correctness checks?
  A:  For problem size: input parameters: (nx_global, ny_global, nz_global) and others
      For performance:  clock and CPU times
      For correctness:  last residual norm

  Q:  Explain each of the metrics in the output that belong to performance
  A:
  // Wall clock time means difference between timestamps of start and finish of computations
  // CPU time is time spent by processor only, excluding time spent for input/output, idle time
  // Solver time is time spent inside of the solution function in contrary to time spent on filling in matrix entries

  Q:  Are any of the performance metrics compounded based on others
  A:  systemSize * #it / solveTime

  Q:  Is this a weak or strong scaling benchmark? Explain.
  A:  //J:I think this is strong scaling since we are increasing the number of processes and changing the local problem size but fixing the global one.
      //J:Though I could be wrong );

  Q:  Do the provided Load Leveler scripts keep the problem size constant? Explain
  A: //Y: Each submission file has different problem size. However in this way we state sizes only for one process. If we summ sizes up we can see that the global size stays the same for every scenario and every number of processes
 //J:Like I said in the questions before it fixes the global problem size but changes the local one!

4.1.2 ###############################
  Q:   Look at the compilers’ help (by issuing icc -help and gcc -help). How many optimization flags are available for each compiler (approximately)?
  A: // Y: --help flag returns hundreds for icc and about 30 for gcc running through powerset of all flags with crdinal nuber 2^n is not duable at all

  Q:  Given how much time it takes to evaluate a combination of compiler flags, would it be realistic to test
all possible combinations of available compiler flags in these compilers?
  A: // Y: Running through powerset of all flags with cardinal nuber 2^n is not doable at all

  Q: Which compiler and optimization flags combination produced the fastest binary?
  A: Y: (For now) the fastest combination (among our tries) is icc with -O3 -unroll flag

4.2.2###############################
  Q: In the video, discuss the difference between Intel’s simd, vector and ivdep #pragma directives. Addi-
tionally, justify your decision to apply the selected #pragma in the particular location on the submitted
source file.
  A:  *simd   : enforces vectorization
      *vector : indicates the compiler that the loop could be vectorized based on his argument
      *ivdep  : ignores data dependencies that restricts vectorization

  Q:  Discuss why you selected the pragma in the code
  A:  ?????
