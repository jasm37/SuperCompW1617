Last assignment ;)

2.1 Submit updated GE with collective commands
2.2 Questions:
    1.  Which patterns were identified and replaced with collective communication?
        A:  Bcast was used to share sizes of problem, scatter was used to distibute parts of problem matrix, gather was used to asssemble solution together.
			In order to replace all communication with non-blocking ones the solution part of the program was changed:
				we iterate over number of ranks, with each new iteration we reduce sze of communicator group exluding ranks with alredy solved parts, also reordered partsof solution so that communication is between obtaining results and propagating them through the next ranks
    2.  Were you able to identify any potential for overlap and used any non-blocking collectives?
        A:  The computation for each next block depends on the results from previous ranks, so no overlaps are possible
			Non-blocking collectives were not used. Furthermore non-blocking communication did not show any improvement
			for asignment with p2p commnication, and non-blocking collectives in the solution part of the program will cause
			even larger overhead, since they require particpation of more ranks.
    3.  Was there any measurable performance or scalability improvement as a result of these changes?
        A:  There is no significant speed up, for large problem sizes about 1% for significant number of processor.
    4.  Is the resulting code easier to understand and maintain after the changes? Why?
        A:  Yes, the reduction of MPI lines/commands makes it easier to read and mantain.
		What needed loop over ranks with separate send and recieve operations now requires onlz one line of code with bcast/gather/scatter.
3.1 Submit updated GE with IO commands
3.2 Questions:
    1.  Which MPI-IO operations were applied to transform the code? Explain choices
        A:  MPI_File_open : Creates a MPI_File pointer to a file address for each rank.
            MPI_File_read_all : Collective read of a selected file and store content in a buffer at each rank.
            MPI_File:read_at_all: Collective read of a file given offsets for each rank(different positions in the file) and store
                                  content in a buffer.
            MPI_File_write_at_all : Collective write of a buffer to a file given offsets for each rank(different positions in the file)
            MPI_File_close :  Closes the selected MPI_File pointer.
            **Observation: all reads an writes are done in binary format
    2.  What is "Data-Sieving" and "2 Phase IO"? How do they help improve IO performance?
        A:
            **Data Sieving: Sources : http://home.agh.edu.pl/~kzajac/ooc_tut/node6.html
                                      http://www.mcs.anl.gov/~thakur/papers/romio-coll.pdf
            It has been determined that many applications need to access a large number of small, noncontiguous pieces
            of data from a file but for a good I/O performance, the size of an I/O request must be large (in the order of megabytes).
            The I/O performance suffers considerably if applications access data by making MANY small I/O requests.
            To reduce the effect of high I/O latency, it is critical to make as few requests to the file system as possible.
            Data Sieving was introduced to solve these kind of problems: Instead of accessing each contiguous portion of the
            data separately, a single contiguous chunk of data starting from the first requested byte up to the last
            requested byte is read into a temporary buffer in memory (sort of a Cache).
            Additionally, the requested portions of data are extracted and placed in the user's buffer.

            A potential problem with this "simple" algorithm is memory requirement. The temporary buffer into which data
            is first read must be as large as the total number of bytes between the first and last byte requested by the user.
            To avoid this problem, the technique described above is performed in parts, reading only as much data at a time
            as defined by a special parameter.
            >>Data Sieving can also be used to write data..
            >>One could argue that most file systems perform sieving in the form of caching
            >>> The advantage of data sieving is that data is always accessed in large chunks,
                although at the cost of reading more data than needed.

            **2 Phase IO: Source: http://www.mcs.anl.gov/~thakur/papers/romio-coll.pdf
            Made with the intention of accessing distributed arrays from files. The local array of each process may be located
            noncontiguously in the file. If each process tries to read each row of its local array individually, the performance
            will be poor due to large number of small IO requests.
            If the  IO access pattern is known to the implementation then data can accessed efficiently by splitting the access into
            two phases:
              I.  Processes access data assuming a distribution in memory that results in each process making one long continuous acces
              II. Processes distribute the data among themselves according to a desired distribution.
            >>The advantage is that making large, contiguous accesses, the IO time is reduced considerably. Although there might be a
              small cost of communication between processes.

    3.  Was the original implementation scalable in terms of IO performance?
        A:  IN terms of IO, the original implementation just loads the whole matrix A and rhs b to rank 0 and proceeds
            to distribute it to the rest of the ranks. This is plotted in the slides for 8192x8192.
            The comparison shows that the time required for the whole procedure remains almost constant while the IO
            approach scales slowly but better than the original.
    4.  Was the original implementation scalable in terms of RAM storage?
        A:  In terms of memory rank 0 always stores the complete array which does not scale properly for large matrices
            In the case of IO, every rank gets a chunk of data from the original file , making a better scaling when more
            processes are added(less memory per process)
    5.  How much of the communication in the application was replaced or eliminated with MPI IO?
        A:  All of input and output commands in rank 0 were replaced by collective MPI IO commands. The respective
            replacement is shown in the slides.
    6.  Were there any performance improvements due to the change to MPI-IO?
        A:  Yes, while doing collective reading or writing of a file, the IO times decreases since the task is distributed
            to all ranks and not to only rank 0 as in the original implementation.
            In the plot in the slides, we only plot IO+computational time since they are the only affected time with
            the addition of the MPI IO commands. So basically the other time remain almost the same except for these.
